{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "979dca89",
   "metadata": {},
   "source": [
    "PHO Vinh-Son 3802052 <br>\n",
    "CHOI Esther 3800370"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cb51c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import codecs\n",
    "import re\n",
    "import os.path\n",
    "\n",
    "import sklearn.naive_bayes as nb\n",
    "from sklearn import svm\n",
    "from sklearn import linear_model as lin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "def conf(Y_Test,Y_hat):\n",
    "    confusion = np.zeros((2,2))\n",
    "    Y_hat = (Y_hat+1)//2\n",
    "    Y_Test = (Y_Test+1)//2\n",
    "    for i in range(Y_Test.size):\n",
    "        confusion[Y_Test[i],Y_hat[i]] +=1\n",
    "    return confusion\n",
    "    \n",
    "def f1(conf):\n",
    "    tp = conf[0,0]\n",
    "    false = conf[0,1] + conf[1,0]\n",
    "    return (tp/(tp+0.5*false))\n",
    "\n",
    "def load_pres(fname):\n",
    "    alltxts = []\n",
    "    alllabs = []\n",
    "    s=codecs.open(fname, 'r','utf-8') # pour régler le codage\n",
    "    while True:\n",
    "        txt = s.readline()\n",
    "        if(len(txt))<5:\n",
    "            break\n",
    "        #\n",
    "        lab = re.sub(r\"<[0-9]*:[0-9]*:(.)>.*\",\"\\\\1\",txt)\n",
    "        txt = re.sub(r\"<[0-9]*:[0-9]*:.>(.*)\",\"\\\\1\",txt)\n",
    "        if lab.count('M') >0:\n",
    "            alllabs.append(-1)\n",
    "        else: \n",
    "            alllabs.append(1)\n",
    "        alltxts.append(txt)\n",
    "    return alltxts,alllabs\n",
    "\n",
    "def load_pres_T(fname):\n",
    "    alltxts_T = []\n",
    "    s=codecs.open(fname, 'r','utf-8') # pour régler le codage\n",
    "    while True:\n",
    "        txt = s.readline()\n",
    "        if(len(txt))<5:\n",
    "            break\n",
    "        txt = re.sub(r\"<[0-9]*:[0-9]*>(.*)\",\"\\\\1\",txt)\n",
    "        alltxts_T.append(txt)\n",
    "    return alltxts_T\n",
    "\n",
    "def crossval(alltxts,alllabs,cv = 5,shuffle = False, seed = 42):\n",
    "    ind = np.array([i for i in range(alllabs.size)])\n",
    "    if shuffle:\n",
    "        np.random.seed(seed)\n",
    "        np.random.shuffle(ind)\n",
    "    splitind = np.array_split(ind, cv)\n",
    "    lcrossval = []\n",
    "    for indices in splitind:\n",
    "        lcrossval.append((alltxts[indices],np.delete(alltxts,indices),alllabs[indices],np.delete(alllabs,indices)))\n",
    "    return lcrossval\n",
    "\n",
    "def gaussianKernel(n):\n",
    "    res = np.ones(2*n+1)\n",
    "    sig = (n+1)/3\n",
    "    for i in range(n):\n",
    "        tmp = np.exp((-(i+1)**2)/(2*sig))\n",
    "        res[n+(i+1)]= tmp\n",
    "        res[n-(i+1)]= tmp\n",
    "    return res #c'est un kernel proportionnel à un \"vrai\" kernel gaussien\n",
    "\n",
    "def smoothing(y_hat,smooth,mode = \"hard\"):\n",
    "    if mode == \"FFT\":\n",
    "        new = np.array(y_hat)\n",
    "        new = (new+1)//2 #on passe en représentation 0, 1.\n",
    "        fftsig = np.fft.fft(new)\n",
    "        tmp = np.fft.fftfreq(new.size)\n",
    "        \n",
    "        for i in range(fftsig.size):\n",
    "            if np.abs(tmp[i]) > smooth[0] :\n",
    "                fftsig[i] =  0\n",
    "        new = np.rint(np.abs(np.fft.ifft(fftsig))).astype(int)\n",
    "        \n",
    "        for i in range(new.size): #si erreurs dans la reconstruction\n",
    "            if new[i] > 1:\n",
    "                new[i] = 1\n",
    "        return 2*new -1 #on repasse en représentation -1,1\n",
    "    \n",
    "    else:    \n",
    "        new = np.array(y_hat).astype('float64')\n",
    "        for k in smooth:\n",
    "            if mode == \"Gaussian\":\n",
    "                ker = gaussianKernel(k)\n",
    "            for i in range(k,new.size-k):\n",
    "                if mode == \"hard\":\n",
    "                    new[i] = np.sign(np.sum(new[i-k:i+k+1])) #equivalent signe(moyenne (...))\n",
    "                elif mode == \"soft\":\n",
    "                    new[i] = np.mean(new[i-k:i+k+1]) #on ne prend pas encore le signe\n",
    "                elif mode == \"Gaussian\":\n",
    "                    new[i] = np.sign(np.sum((new[i-k:i+k+1]*ker)))\n",
    "        return np.sign(new).astype('int32')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81bddb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_pred_param(df,mf,b,smooth,smoothmode = \"hard\", fname = \"corpus.tache1.learn.utf8\",cv_fold = 5,vectorizer = CountVectorizer, classifier = lin.LogisticRegression):\n",
    "    #extraction\n",
    "    alltxts,alllabs = load_pres(fname)\n",
    "    alltxts = np.array(alltxts)\n",
    "    alllabs = np.array(alllabs)\n",
    "    \n",
    "    lcrossval = crossval(alltxts,alllabs,cv = cv_fold)\n",
    "    \n",
    "    liste_score = []\n",
    "    liste_score_untouched = []\n",
    "    for X_Test, X_Train, Y_Test, Y_Train in lcrossval:\n",
    "\n",
    "        #init\n",
    "        vectorizer2 = vectorizer(analyzer='word',max_df = df, max_features = mf, lowercase = False)\n",
    "        X_VTr = vectorizer2.fit_transform(X_Train)\n",
    "        X_VTe = vectorizer2.transform(X_Test)\n",
    "        clf = classifier(max_iter = 300)\n",
    "\n",
    "        #fit\n",
    "        clf.fit(X_VTr, Y_Train)  \n",
    "        #test\n",
    "        proba = clf.predict_proba(X_VTe)\n",
    "\n",
    "        #adjust\n",
    "        Y_hat_b = []\n",
    "        for i in range(Y_Test.size):\n",
    "            if proba[i][0] > b: #hyperparam Biais\n",
    "                Y_hat_b.append(-1)\n",
    "            else:\n",
    "                Y_hat_b.append(1)\n",
    "        Y_hat_b = np.array(Y_hat_b)\n",
    "\n",
    "        #score before smoothing\n",
    "        conf_b = np.zeros((2,2))\n",
    "        Y_hat_b_o = (Y_hat_b+1)//2\n",
    "        Y_Test_o = (Y_Test+1)//2\n",
    "        for i in range(Y_Test.size):\n",
    "            conf_b[Y_Test_o[i],Y_hat_b_o[i]] +=1\n",
    "        liste_score_untouched.append(f1(conf_b))\n",
    "\n",
    "        #smoothing\n",
    "        Y_hat_b = smoothing(Y_hat_b,smooth, mode = smoothmode)\n",
    "\n",
    "        #scoring\n",
    "        conf_b = np.zeros((2,2))\n",
    "        Y_hat_b = (Y_hat_b+1)//2\n",
    "        Y_Test = (Y_Test+1)//2\n",
    "        for i in range(Y_Test.size):\n",
    "            conf_b[Y_Test[i],Y_hat_b[i]] +=1\n",
    "        liste_score.append(f1(conf_b))\n",
    "    liste_score = np.array(liste_score)\n",
    "    liste_score_untouched = np.array(liste_score_untouched)\n",
    "\n",
    "    return liste_score, liste_score_untouched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70abb205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8067196267531151\n"
     ]
    }
   ],
   "source": [
    "#KZF modifié \n",
    "df,mf,b,smooth = (0.11, 10000, 0.144, [1, 2, 3, 4, 5, 6, 7, 8, 2])\n",
    "l1, l2 = score_pred_param(df,mf,b,smooth)\n",
    "print(l1.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5edf33c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Score avec KZF modifié\n",
    "#(0.18, 10000, 0.134, [1, 2, 3, 4, 5, 6, 7, 8, 2])\n",
    "#0.8034365297795751\n",
    "\n",
    "#(0.11, 10000, 0.144, [1, 2, 3, 4, 5, 6, 7, 8, 2])\n",
    "#0.8067196267531151"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "016f25b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7969941427268449\n"
     ]
    }
   ],
   "source": [
    "#KZF -> Utiliser smoothmode = \"soft\"\n",
    "df,mf,b,smooth = (0.11, 10000, 0.144, [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1])\n",
    "l1, l2 = score_pred_param(df,mf,b,smooth,smoothmode = \"soft\")\n",
    "print(l1.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6ab0dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7650996393937654\n"
     ]
    }
   ],
   "source": [
    "#Gaussian \n",
    "df,mf,b,smooth = (0.18, 10000, 0.134, [14])\n",
    "l1, l2 = score_pred_param(df,mf,b,smooth,smoothmode = \"Gaussian\")\n",
    "print(l1.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "134d175b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7801624462719565\n"
     ]
    }
   ],
   "source": [
    "#FFT\n",
    "df,mf,b,smooth = (0.18, 10000, 0.134, [0.035])\n",
    "l1, l2 = score_pred_param(df,mf,b,smooth,smoothmode = \"FFT\")\n",
    "print(l1.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68372ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(df,mf,b,smooth,smoothmode = \"hard\", fnameL = \"corpus.tache1.learn.utf8\",fnameT = \"corpus.tache1.test.utf8\",fnameP = \"pred_presidents.txt\",vectorizer = CountVectorizer, classifier = lin.LogisticRegression):\n",
    "    alltxts_T = load_pres_T(fnameT)\n",
    "    alltxts_T = np.array(alltxts_T)\n",
    "    \n",
    "    alltxts,alllabs = load_pres(fnameL)\n",
    "    alltxts = np.array(alltxts)\n",
    "    alllabs = np.array(alllabs)\n",
    "    \n",
    "    vectorizer2 = vectorizer(analyzer='word',max_df = df, max_features = mf, lowercase = False)\n",
    "    X_VTr = vectorizer2.fit_transform(alltxts)\n",
    "    X_VTe = vectorizer2.transform(alltxts_T)\n",
    "    clf = classifier(max_iter = 300)\n",
    "    \n",
    "    #fit\n",
    "    clf.fit(X_VTr, alllabs)\n",
    "    #predict\n",
    "    proba = clf.predict_proba(X_VTe)\n",
    "    #adjust\n",
    "    Y_hat_b = []\n",
    "    for i in range(X_VTe.shape[0]):\n",
    "        if proba[i][0] > b: #hyperparam Biais\n",
    "            Y_hat_b.append(-1)\n",
    "        else:\n",
    "            Y_hat_b.append(1)\n",
    "    Y_hat_b = np.array(Y_hat_b)\n",
    "    \n",
    "    #smoothing\n",
    "    Y_hat_b = smoothing(Y_hat_b,smooth, mode = smoothmode)\n",
    "        \n",
    "    #storing\n",
    "    f = open(fnameP, \"w\")\n",
    "    for i in Y_hat_b:\n",
    "        f.write(str(i)+\"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abee4917",
   "metadata": {},
   "outputs": [],
   "source": [
    "df,mf,b,smooth = (0.19,13250,0.145,[1,2,3,4,5,6,7,8,2])\n",
    "prediction(df,mf,b,smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d116fb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
